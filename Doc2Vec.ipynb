{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def read_dataset(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset.iloc[:,1], dataset.iloc[:,2], random_state=0, test_size=0.1,\n",
    "                                                train_size = 0.4,\n",
    "                                               stratify = dataset.iloc[:,2])\n",
    "    x_train = label_sentences(x_train, 'Train')\n",
    "    x_test = label_sentences(x_test, 'Test')\n",
    "    all_data = x_train + x_test\n",
    "    return x_train, x_test, y_train, y_test, all_data\n",
    "\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the LabeledSentence method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the review.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.LabeledSentence(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "\n",
    "def get_vectors(doc2vec_model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = doc2vec_model.docvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def train_doc2vec(corpus):\n",
    "    logging.info(\"Building Doc2Vec vocabulary\")\n",
    "    d2v = doc2vec.Doc2Vec(min_count=10,  # Ignores all words with total frequency lower than this\n",
    "                          window=10,  # The maximum distance between the current and predicted word within a sentence\n",
    "                          vector_size=300,  # Dimensionality of the generated feature vectors\n",
    "                          workers=10,  # Number of worker threads to train the model\n",
    "                          alpha=0.025,  # The initial learning rate\n",
    "                          min_alpha=0.00025,  # Learning rate will linearly drop to min_alpha as training progresses\n",
    "                          dm=1)  # dm defines the training algorithm. If dm=1 means 'distributed memory' (PV-DM)\n",
    "                                 # and dm =0 means 'distributed bag of words' (PV-DBOW)\n",
    "    d2v.build_vocab(corpus)\n",
    "\n",
    "    logging.info(\"Training Doc2Vec model\")\n",
    "    # 10 epochs take around 10 minutes on my machine (i7), if you have more time/computational power make it 20\n",
    "    for epoch in range(10):\n",
    "        logging.info('Training iteration #{0}'.format(epoch))\n",
    "        d2v.train(corpus, total_examples=d2v.corpus_count, epochs=1)\n",
    "        # shuffle the corpus\n",
    "        random.shuffle(corpus)\n",
    "        # decrease the learning rate\n",
    "        d2v.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        d2v.min_alpha = d2v.alpha\n",
    "\n",
    "    logging.info(\"Saving trained Doc2Vec model\")\n",
    "    d2v.save(\"d2v.model\")\n",
    "    return d2v\n",
    "\n",
    "\n",
    "def train_classifier(d2v, training_vectors, training_labels):\n",
    "    logging.info(\"Classifier training\")\n",
    "    train_vectors = get_vectors(d2v, len(training_vectors), 300, 'Train')\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_vectors, np.array(training_labels))\n",
    "    training_predictions = model.predict(train_vectors)\n",
    "    logging.info('Training predicted classes: {}'.format(np.unique(training_predictions)))\n",
    "    logging.info('Training accuracy: {}'.format(accuracy_score(training_labels, training_predictions)))\n",
    "    logging.info('Training F1 score: {}'.format(f1_score(training_labels, training_predictions, average='weighted')))\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_classifier(d2v, classifier, testing_vectors, testing_labels):\n",
    "    logging.info(\"Classifier testing\")\n",
    "    test_vectors = get_vectors(d2v, len(testing_vectors), 300, 'Test')\n",
    "    testing_predictions = classifier.predict(test_vectors)\n",
    "    logging.info('Testing predicted classes: {}'.format(np.unique(testing_predictions)))\n",
    "    logging.info('Testing accuracy: {}'.format(accuracy_score(testing_labels, testing_predictions)))\n",
    "    logging.info('Testing F1 score: {}'.format(f1_score(testing_labels, testing_predictions, average='weighted')))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x_train, x_test, y_train, y_test, all_data = read_dataset('dataset.csv')\n",
    "    d2v_model = train_doc2vec(all_data)\n",
    "    classifier = train_classifier(d2v_model, x_train, y_train)\n",
    "    test_classifier(d2v_model, classifier, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
